1.主机环境
centos6.7, 1024M内存, jdk1.7.0_79, VirtualBox 4.3.20
VBoxManage internalcommands sethduuid D:\path\ubuntu.vdi 为每台虚拟机生成新的uuid
spark1.6.1 http://mirror.bit.edu.cn/apache/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz
hadoop2.6.4 http://apache.fayea.com/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz

2.网络规划:
192.168.1.120 master
192.168.1.121 slave1
192.168.1.122 slave2
192.168.1.123 slave2

3.网络环境配置:
VirtualBox桥接，在VirtualBox设置中选网络，连接方式选桥接网卡，界面名称Dell Wireless 1707 802.11b/g/n（当前正在连接的无线网）.
在本地网络连接中查看所选的无限网连接的VirtualBox Bridged Networking Driver是否勾选。
启动虚拟机，此时每台虚拟机会生成一个新的mac地址：
[root@slave1 ~]# more /etc/udev/rules.d/70-persistent-net.rules 
# This file was automatically generated by the /lib/udev/write_net_rules
# program, run by the persistent-net-generator.rules rules file.
#
# You can modify it, as long as you keep each rule on a single
# line, and change only the value of the NAME= key.

# PCI device 0x8086:0x100e (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="08:00:27:12:0b:ff", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"

# PCI device 0x8086:0x100e (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="08:00:27:c0:9f:ff", ATTR{type}=="1", KERNEL=="eth*", NAME="eth1"
这里eth0的mac地址是被复制机器的mac地址，eth1是新生成的，把eth0这行注释掉，把eth1改为eth0。
[root@slave1 ~]# more /etc/udev/rules.d/70-persistent-net.rules 
# This file was automatically generated by the /lib/udev/write_net_rules
# program, run by the persistent-net-generator.rules rules file.
#
# You can modify it, as long as you keep each rule on a single
# line, and change only the value of the NAME= key.

# PCI device 0x8086:0x100e (e1000)
# SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="08:00:27:12:0b:ff", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"

# PCI device 0x8086:0x100e (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="08:00:27:c0:9f:ff", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"
设置虚拟机固定ip地址和本地无线连接处于同一个网段，并把ifcfg-eth0的HWADDR设为上面新的mac地址
[root@master ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0
HWADDR=08:00:27:88:ec:ce
TYPE=Ethernet
UUID=674041f2-e22f-4dad-ba79-7fe4ab92388d
ONBOOT=yes
NM_CONTROLLED=yes
#BOOTPROTO=dhcp
IPADDR=192.168.1.120
NETMASK=255.255.255.0
GATEWAY=192.168.1.1
设置hostname
[root@master ~]# more /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=master
设置hosts
[root@master ~]# more /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.120	master
192.168.1.121	slave1
192.168.1.122	slave2
192.168.1.123	slave3
网络配置完成，应该实现虚拟机间互访，虚拟机和主机互访

4.设置ssh免密码登录
[root@master ~]# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
[root@master ~]# cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
//下面scp的时候需要输入密码
[root@master ~]# scp .ssh/authorized_keys root@slave1:/root/.ssh/
[root@master ~]# scp .ssh/authorized_keys root@slave2:/root/.ssh/
[root@master ~]# scp .ssh/authorized_keys root@slave3:/root/.ssh/
至此master可以免密码登录本机和slave1、slave2、slave3

5.spark Standalone模式部署
[root@master ~]# cd /opt/spark-1.6.1-bin-hadoop2.6/conf
[root@master conf]# mv slaves.template slaves
[root@master conf]# more slaves
# A Spark Worker will be started on each of the machines listed below.
slave1
slave2
slave3
[root@master conf]# mv spark-env.sh.template spark-env.sh
设置JAVA_HOME
[root@master conf]# tail spark-env.sh
# Generic options for the daemons used in the standalone deploy mode
# - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
# - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
# - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
# - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
JAVA_HOME=/usr/local/java/jdk1.7.0_79
复制到slave
[root@master opt]# scp -r spark-1.6.1-bin-hadoop2.6 root@slave1:/opt/
[root@master opt]# scp -r spark-1.6.1-bin-hadoop2.6 root@slave2:/opt/
[root@master opt]# scp -r spark-1.6.1-bin-hadoop2.6 root@slave3:/opt/
启动集群
[root@master opt]# cd spark-1.6.1-bin-hadoop2.6/
[root@master spark-1.6.1-bin-hadoop2.6]# sbin/start-all.sh 
starting org.apache.spark.deploy.master.Master, logging to /opt/spark-1.6.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.master.Master-1-master.out
failed to launch org.apache.spark.deploy.master.Master:
full log in /opt/spark-1.6.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.master.Master-1-master.out
slave3: starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark-1.6.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-slave3.out
slave2: starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark-1.6.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-slave2.out
slave1: starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark-1.6.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-slave1.out
slave3: failed to launch org.apache.spark.deploy.worker.Worker:
slave3: full log in /opt/spark-1.6.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-slave3.out
slave2: failed to launch org.apache.spark.deploy.worker.Worker:
slave2: full log in /opt/spark-1.6.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-slave2.out
slave1: failed to launch org.apache.spark.deploy.worker.Worker:
slave1: full log in /opt/spark-1.6.1-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-slave1.out
这里日志说Worker启动不成功，但是到slave上用jps看发现Worker是启动正常的，log里也没有报错。
访问master主页：http://192.168.1.120:8080/，可以看到Worker都是Alive
[root@master spark-1.6.1-bin-hadoop2.6]# bin/spark-shell --master spark://master:7077
连接成功
[root@master spark-1.6.1-bin-hadoop2.6]# ./bin/spark-submit \
> --class org.apache.spark.examples.SparkPi \
> --master spark://master:7077 \
> lib/spark-examples-1.6.1-hadoop2.6.0.jar \
> 100
执行SparkPi成功，安装OK!
在SPARK_HOME下都会有一个work目录，里面存放着每个application对应的application日志。logs目录下是对应的worker日志

6.hadoop部署
[root@master ~]# ls /opt/
hadoop-2.6.4  spark-1.6.1-bin-hadoop2.6
配置文件单独出来：
[root@master ~]# cd hadoop_conf/
[root@master hadoop_conf]# ls
core-site.xml  hadoop-env.sh  hdfs-site.xml  mapred-site.xml  master  slaves  yarn-env.sh  yarn-site.xml
[root@master hadoop_conf]# cat master 
master

[root@master hadoop_conf]# cat slaves 
slave1
slave2
slave3

[root@master hadoop_conf]# cat core-site.xml
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://master:9000</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
        <value>file:/opt/hadoopData/tmp</value>
    </property>
</configuration>

[root@master hadoop_conf]# more hdfs-site.xml
<configuration>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/opt/hadoopData/dfs/nameNode</value>
	</property>
	<property>              
		<name>dfs.datanode.data.dir</name>
        <value>file:/opt/hadoopData/dfs/dataNode</value>
    </property>
</configuration>

[root@master hadoop_conf]# cat mapred-site.xml
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
</configuration>

[root@master hadoop_conf]# cat yarn-site.xml
<configuration>

<!-- Site specific YARN configuration properties -->
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>master</value>
	</property>
	<property>
		<name>yarn.nodemanager.local-dirs</name>
		<value>file:/opt/hadoopData/yarn</value>
    </property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
</configuration>


设置hadoop-env.sh和yarn-env.sh中的JAVA_HOME

在/etc/profile中添加：
export HADOOP_CONF_DIR=/root/hadoop_conf
将 hadoop_conf复制到另外三台slave上，并设置HADOOP_CONF_DIR
全部重启

格式化HDFS
[root@master hadoop-2.6.4]# bin/hdfs namenode -format
启动HDFS
[root@master hadoop-2.6.4]# sbin/start-dfs.sh
jps查看，master上有NameNode和SecondaryNameNode，slave上有DataNode
[root@master hadoop-2.6.4]# bin/hdfs dfs -mkdir /user
[root@master hadoop-2.6.4]# bin/hdfs dfs -mkdir /user/root
如果没有指出绝对路径则默认在/user/root下（因为使用root账户登录），如果要放在别的目录下需指明路径
[root@master hadoop-2.6.4]# bin/hdfs dfs -put etc/hadoop input
[root@master hadoop-2.6.4]# bin/hdfs dfs -ls input
HDFS启动完成

[root@master hadoop-2.6.4]# sbin/start-yarn.sh
jps发现 ResourceManager 没有起来，查看日志，抛出下面的异常：
java.lang.IllegalStateException: Queue configuration missing child queue names for root
从etc/hadoop下复制
[root@master hadoop-2.6.4]# cp etc/hadoop/capacity-scheduler.xml ~/hadoop_conf/
重新执行start-yarn.sh脚本，master上的ResourceManager和slave上的NodeManager都启动
yarn启动完成

执行个任务：
[root@master hadoop-2.6.4]# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar grep input output 'dfs[a-z.]+'
[root@master hadoop-2.6.4]# bin/hdfs dfs -ls output
Found 2 items
-rw-r--r--   3 root supergroup          0 2016-03-19 17:45 output/_SUCCESS
-rw-r--r--   3 root supergroup          0 2016-03-19 17:45 output/part-r-00000

NameNode http://192.168.1.120:50070/
ResourceManager http://192.168.1.120:8088/
MapReduce JobHistory Server 

7.两种使用方式：
7.1.standalone + hdfs，此时不需要启动yarn
[root@master spark-1.6.1-bin-hadoop2.6]# bin/spark-shell --master=spark://master:7077
scala> val hdfsFile = sc.textFile("hdfs://master:9000/user/root/input/*")
scala> hdfsFile.flatMap(line=>line.split(" ")).map(word=>(word, 1)).reduceByKey(_+_).saveAsTextFile("hdfs://master:9000/user/root/output")

[root@master hadoop-2.6.4]# bin/hdfs dfs -ls output
Found 7 items
-rw-r--r--   3 root supergroup          0 2016-03-19 20:18 output/_SUCCESS
-rw-r--r--   3 root supergroup        967 2016-03-19 20:18 output/part-00000
-rw-r--r--   3 root supergroup        731 2016-03-19 20:18 output/part-00001
-rw-r--r--   3 root supergroup        872 2016-03-19 20:18 output/part-00002
-rw-r--r--   3 root supergroup        549 2016-03-19 20:18 output/part-00003
-rw-r--r--   3 root supergroup        530 2016-03-19 20:18 output/part-00004
-rw-r--r--   3 root supergroup        885 2016-03-19 20:18 output/part-00005

[root@master hadoop-2.6.4]# bin/hdfs dfs -cat output/part-00000
('zookeeper',2)
(Unless,5)
(<value>10000</value>,1)
(<name>hadoop.kms.authentication.kerberos.name.rules</name>,1)
(affects,2)
(Security,1)
...

7.2.Spark on yarn，启动yarn，不需要启动spark(即不需要执行$SPARK_HOME/bin/start-all.sh)，直接提交任务
[root@master spark-1.6.1-bin-hadoop2.6]# ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster lib/spark-examples-1.6.1-hadoop2.6.0.jar 10
